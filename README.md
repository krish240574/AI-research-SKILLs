# ğŸ“ AI Research Skills Library

> **The most comprehensive open-source library of AI research skills for AI models**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Skills](https://img.shields.io/badge/Skills-7-blue.svg)](.)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

Empower AI models (Claude, GPT, Gemini, and others) with deep expertise across the entire AI research lifecycleâ€”from model architecture to production deployment. Each skill provides comprehensive documentation, real-world examples, and battle-tested best practices.

**Works with**: Any AI assistant that supports skills or knowledge injection.

---

## ğŸŒŸ Why This Project?

**Problem**: AI researchers juggle 100+ frameworks, libraries, and tools. Documentation is scattered. Best practices are tribal knowledge.

**Solution**: Curated, comprehensive skills that make any AI model your expert research assistant.

**Impact**: 
- âš¡ **10x faster prototyping** - Skip docs hunting, get straight to coding
- ğŸ¯ **Production-ready patterns** - Learn from real GitHub issues & releases
- ğŸ¤ **Community-driven** - Open-source knowledge base that grows with the field

---

## ğŸ“š Available Skills (7/70 roadmap)

**Quality over quantity**: Each skill provides comprehensive, expert-level guidance with real code examples, troubleshooting guides, and production-ready workflows.

### ğŸ¯ Fine-Tuning (4 skills)
- **[Axolotl](03-fine-tuning/axolotl/)** - YAML-based fine-tuning with 100+ models (151 lines, comprehensive)
- **[TRL](03-fine-tuning/trl-fine-tuning/)** - Transformer Reinforcement Learning (56 lines)
- **[LLaMA-Factory](03-fine-tuning/llama-factory/)** - WebUI no-code fine-tuning (73 lines)
- **[Unsloth](03-fine-tuning/unsloth/)** - 2x faster QLoRA fine-tuning (70 lines)

### ğŸ“ Post-Training (1 skill)
- **[GRPO-RL-Training](06-post-training/grpo-rl-training/)** - Group Relative Policy Optimization with TRL (569 lines, **gold standard**)

### âš¡ Distributed Training (2 skills)
- **[DeepSpeed](08-distributed-training/deepspeed/)** - Microsoft's ZeRO optimization (132 lines)
- **[PyTorch FSDP](08-distributed-training/pytorch-fsdp/)** - Fully Sharded Data Parallel (119 lines)

---

## ğŸš€ Quick Start

### For AI Users

1. **Clone this repository**:
```bash
git clone https://github.com/YOUR_USERNAME/claude-ai-research-skills.git
cd claude-ai-research-skills
```

2. **Browse skills** - Each directory contains:
   - `SKILL.md` - Expert-level guidance (100-600 lines)
   - `references/` - Supporting documentation
   - Real code examples, workflows, and troubleshooting

3. **Use with your AI assistant**:
   - **Upload as skill**: Package into `.zip` and upload to any AI assistant that supports custom skills
   - **Attach as context**: Copy-paste `SKILL.md` content or attach as file
   - **RAG knowledge**: Inject into your custom agent's knowledge base
   - **Claude Code**: Point to skill directory for native integration

### For Skill Creators

1. **Read [CONTRIBUTING.md](CONTRIBUTING.md)** - Step-by-step guide
2. **Use [SKILL_TEMPLATE.md](SKILL_TEMPLATE.md)** - Copy-paste scaffold
3. **Run validation**: `python scripts/validate_skill.py your-skill/`
4. **Submit PR** - We review within 48 hours

---

## ğŸ“– Skill Structure

Each skill follows a battle-tested format for maximum usefulness:

```
skill-name/
â”œâ”€â”€ SKILL.md                    # Quick reference (50-150 lines)
â”‚   â”œâ”€â”€ Metadata (name, description, version)
â”‚   â”œâ”€â”€ When to use this skill
â”‚   â”œâ”€â”€ Quick patterns & examples
â”‚   â””â”€â”€ Links to references
â”‚
â”œâ”€â”€ references/                 # Deep documentation (300KB+)
â”‚   â”œâ”€â”€ README.md              # From GitHub/official docs
â”‚   â”œâ”€â”€ api.md                 # API reference
â”‚   â”œâ”€â”€ tutorials.md           # Step-by-step guides
â”‚   â”œâ”€â”€ issues.md              # Real GitHub issues & solutions
â”‚   â”œâ”€â”€ releases.md            # Version history & breaking changes
â”‚   â””â”€â”€ file_structure.md      # Codebase navigation
â”‚
â”œâ”€â”€ scripts/                    # Helper scripts (optional)
â””â”€â”€ assets/                     # Templates & examples (optional)
```

**Quality Standards**:
- âœ… 300KB+ documentation from official sources
- âœ… Real GitHub issues & solutions (when available)
- âœ… Code examples with language detection
- âœ… Version history & breaking changes
- âœ… Links to official docs

---

## ğŸ—ºï¸ Roadmap

### âœ… Phase 1: Training Foundation (COMPLETE - 15 skills)
**Status**: 70% of training lifecycle covered

### ğŸš§ Phase 2: Complete Training Stack (Weeks 1-2) - 5 Skills
**Priority**: HIGH - Finish what we started

- [ ] **OpenRLHF** - Open-source RLHF implementation
- [ ] **VERL** - VolcEngine RL for LLMs  
- [ ] **Flash Attention** - Tri Dao's kernel optimization
- [ ] **bitsandbytes** - 8-bit training/inference
- [ ] **RWKV** - State-space model alternative

### ğŸ¯ Phase 3: Inference & Serving (Weeks 3-4) - 6 Skills
**Priority**: CRITICAL - Enable production deployment

- [ ] **vLLM** - PagedAttention, continuous batching
- [ ] **TensorRT-LLM** - NVIDIA inference optimization
- [ ] **llama.cpp** - CPU/edge inference
- [ ] **SGLang** - Fast structured generation
- [ ] **GPTQ** - Post-training quantization
- [ ] **AWQ** - Activation-aware quantization

### ğŸ“Š Phase 4: Evaluation & Data (Weeks 5-6) - 5 Skills
**Priority**: HIGH - Complete research lifecycle

- [ ] **lm-evaluation-harness** - EleutherAI benchmarks
- [ ] **HELM** - Stanford evaluation
- [ ] **AlpacaEval** - Instruction-following eval
- [ ] **Ray Data** - Distributed data processing
- [ ] **HuggingFace Datasets** - Dataset management

### ğŸ“ˆ Phase 5: MLOps (Weeks 7-8) - 4 Skills
**Priority**: MEDIUM-HIGH - Production readiness

- [ ] **Weights & Biases** - Experiment tracking
- [ ] **MLflow** - Open-source MLOps
- [ ] **TensorBoard** - Visualization
- [ ] **HuggingFace Hub** - Model registry

### ğŸ¤– Phase 6: Applications (Weeks 9-12) - 12 Skills
**Priority**: CRITICAL - Enable AI applications

**Agents**:
- [ ] **LangChain** - Most popular agent framework
- [ ] **LlamaIndex** - Data-focused agents
- [ ] **CrewAI** - Multi-agent collaboration
- [ ] **AutoGPT** - Autonomous agents

**RAG**:
- [ ] **Pinecone** - Vector database
- [ ] **ChromaDB** - Open-source vector DB
- [ ] **LlamaIndex RAG** - RAG pipelines
- [ ] **Sentence Transformers** - Embeddings

**Tools**:
- [ ] **DSPy** - Prompt optimization
- [ ] **Instructor** - Structured output
- [ ] **LangSmith** - Observability
- [ ] **Guardrails AI** - Output validation

### ğŸ”¬ Phase 7: Specialized (Weeks 13-16) - 8 Skills
**Priority**: MEDIUM - Cutting-edge techniques

- [ ] **LLaVA** - Vision-language models
- [ ] **Whisper** - Speech-to-text
- [ ] **Stable Diffusion** - Image generation
- [ ] **MoE Training** - Mixture of Experts
- [ ] **Model Merging** - mergekit
- [ ] **Long Context** - RoPE extensions
- [ ] **Speculative Decoding**
- [ ] **Distillation**

**Target**: 70 comprehensive skills by month 6

---

## ğŸ¤ Contributing

We welcome contributions from the AI research community! 

### Ways to Contribute

1. **Add a new skill** - See [CONTRIBUTING.md](CONTRIBUTING.md)
2. **Improve existing skills** - Update docs, add examples
3. **Report issues** - Outdated info, broken links
4. **Share feedback** - What skills do you need?

### Quick Contribution Guide

```bash
# 1. Fork & clone
git clone https://github.com/YOUR_USERNAME/claude-ai-research-skills.git

# 2. Create skill from template
cp SKILL_TEMPLATE.md 12-inference-serving/vllm/SKILL.md

# 3. Fill in documentation
# ... edit SKILL.md and add references/

# 4. Validate
python scripts/validate_skill.py 12-inference-serving/vllm/

# 5. Submit PR
git checkout -b add-vllm-skill
git add 12-inference-serving/vllm/
git commit -m "Add vLLM inference skill"
git push origin add-vllm-skill
```

**Recognition**: All contributors are featured in our [Contributors Hall of Fame](CONTRIBUTORS.md) ğŸŒŸ

---

## ğŸ“Š Project Stats

| Metric | Current | Target (6 months) |
|--------|---------|-------------------|
| **Skills** | 7 (high-quality) | 70 |
| **Avg Lines/Skill** | 138 lines | 300+ lines |
| **Gold Standard Skills** | 1 (GRPO: 569 lines) | 20+ |
| **Contributors** | 1 | 100+ |
| **Coverage** | Fine-tuning (strong) | Full Lifecycle |

**Philosophy**: Quality > Quantity. Each skill must provide real value with comprehensive guidance, not just links to docs.

---

## ğŸ—ï¸ Repository Structure

```
claude-ai-research-skills/
â”œâ”€â”€ README.md                    â† You are here
â”œâ”€â”€ CONTRIBUTING.md              â† Contribution guide
â”œâ”€â”€ SKILL_TEMPLATE.md            â† Skill scaffold
â”œâ”€â”€ PROJECT_ANALYSIS.md          â† Strategic roadmap
â”œâ”€â”€ RESEARCH_QUESTIONNAIRE.md    â† Coverage checklist
â”‚
â”œâ”€â”€ 01-model-architecture/       (2 skills)
â”œâ”€â”€ 02-tokenization/             (1 skill)
â”œâ”€â”€ 03-fine-tuning/              (4 skills âœ“)
â”œâ”€â”€ 04-peft/                     (1 skill âœ“)
â”œâ”€â”€ 05-data-processing/          (1 skill)
â”œâ”€â”€ 06-post-training/            (planned)
â”œâ”€â”€ 07-safety-alignment/         (1 skill)
â”œâ”€â”€ 08-distributed-training/     (3 skills)
â”œâ”€â”€ 09-infrastructure/           (2 skills)
â”œâ”€â”€ 10-optimization/             (planned)
â”œâ”€â”€ 11-evaluation/               (planned)
â”œâ”€â”€ 12-inference-serving/        (planned - CRITICAL)
â”œâ”€â”€ 13-mlops/                    (planned)
â”œâ”€â”€ 14-agents/                   (planned - CRITICAL)
â”œâ”€â”€ 15-rag/                      (planned - CRITICAL)
â”œâ”€â”€ 16-prompt-engineering/       (planned)
â”œâ”€â”€ 17-observability/            (planned)
â”œâ”€â”€ 18-multimodal/               (planned)
â””â”€â”€ 19-emerging-techniques/      (planned)
```

---

## ğŸ¯ Use Cases

### For Researchers
"I need to fine-tune Llama 3 with custom data"
â†’ **03-fine-tuning/axolotl/** - YAML configs, 100+ model support

### For ML Engineers  
"How do I optimize inference latency?"
â†’ **12-inference-serving/vllm/** (coming soon) - PagedAttention, batching

### For Students
"I want to learn how transformers work"
â†’ **01-model-architecture/litgpt/** - Clean implementations

### For Teams
"We need to scale training to 100 GPUs"
â†’ **08-distributed-training/deepspeed/** - ZeRO stages, 3D parallelism

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

**Note**: Individual skills may reference libraries with different licenses. Please check each project's license before use.

---

## ğŸ™ Acknowledgments

Built with:
- ğŸ’œ **[Claude Code](https://www.claude.com/product/claude-code)** - AI pair programming
- âš¡ **[Skill Seeker](https://github.com/yusufkaraaslan/Skill_Seekers)** - Automated doc scraping
- ğŸ¤— **Open Source AI Community** - For amazing tools and docs

Special thanks to:
- EleutherAI, HuggingFace, NVIDIA, Lightning AI, Meta AI, Anthropic
- All researchers who maintain excellent documentation

---

## ğŸ“ Get Involved

- **ğŸ› Issues**: [Report bugs or request skills](https://github.com/YOUR_USERNAME/claude-ai-research-skills/issues)
- **ğŸ’¬ Discussions**: [Ask questions & share ideas](https://github.com/YOUR_USERNAME/claude-ai-research-skills/discussions)
- **ğŸŒŸ Star**: Help others discover this project
- **ğŸ”€ Fork**: Build your own skill collection

---

## ğŸ“ˆ Recent Updates

**November 6, 2025** - v0.2.0
- âœ… Added 10 skills from GitHub (Megatron-Core, Lightning, Ray Train, etc.)
- âœ… Improved skill structure with comprehensive references
- âœ… Created strategic roadmap to 70 skills
- âœ… Added contribution guidelines

**October 2025** - v0.1.0
- ğŸ‰ Initial release with 5 fine-tuning skills

---

<p align="center">
  <strong>Help us build the definitive AI research skill library</strong><br>
  <a href="CONTRIBUTING.md">Contribute</a> â€¢
  <a href="PROJECT_ANALYSIS.md">Roadmap</a> â€¢
  <a href="https://github.com/YOUR_USERNAME/claude-ai-research-skills/discussions">Community</a>
</p>

<p align="center">
  Made with ğŸ’œ by the AI research community
</p>
