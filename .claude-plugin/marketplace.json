{
  "name": "ai-research-skills",
  "owner": {
    "name": "Orchestra Research"
  },
  "metadata": {
    "description": "Comprehensive library of 76 AI research engineering skills enabling autonomous AI research from hypothesis to experimental verification, covering model architectures, training pipelines, optimization, evaluation, and deployment",
    "version": "1.0.0"
  },
  "plugins": [
    {
      "name": "model-architecture",
      "source": "./01-model-architecture",
      "description": "LLM architectures and implementations. Includes LitGPT (20+ models like Llama, Gemma, Phi), Mamba (O(n) state-space models), NanoGPT (educational GPT-2), and RWKV (RNN-Transformer hybrid). Use when implementing, training, or understanding transformer and alternative architectures."
    },
    {
      "name": "tokenization",
      "source": "./02-tokenization",
      "description": "Text tokenization for LLMs. Includes HuggingFace Tokenizers (Rust-based, 1GB in <20s) and SentencePiece (language-independent, multilingual). Use when training custom tokenizers, handling multilingual text, or optimizing tokenization performance."
    },
    {
      "name": "fine-tuning",
      "source": "./03-fine-tuning",
      "description": "LLM fine-tuning frameworks. Includes Axolotl (YAML configs, 100+ models), LLaMA-Factory (WebUI no-code), PEFT (LoRA/QLoRA, 25+ methods), and Unsloth (2-5x faster, 50-80% less memory). Use when fine-tuning models with LoRA, QLoRA, or full fine-tuning."
    },
    {
      "name": "mechanistic-interpretability",
      "source": "./04-mechanistic-interpretability",
      "description": "Neural network interpretability tools. Includes TransformerLens (circuit analysis), SAELens (sparse autoencoders), NNSight (activation patching), and pyvene (intervention library). Use when analyzing model internals, finding circuits, or understanding how models compute."
    },
    {
      "name": "data-processing",
      "source": "./05-data-processing",
      "description": "Data curation and processing at scale. Includes NeMo Curator (GPU-accelerated deduplication, quality filtering, PII redaction) and Ray Data (distributed processing, streaming execution). Use when preparing training datasets or processing large-scale data."
    },
    {
      "name": "post-training",
      "source": "./06-post-training",
      "description": "RLHF and preference alignment. Includes TRL (SFT, DPO, PPO, GRPO), GRPO (Group Relative Policy Optimization), OpenRLHF (Ray+vLLM acceleration), and SimPO (reference-free alignment). Use when aligning models with human preferences or training reward models."
    },
    {
      "name": "safety-alignment",
      "source": "./07-safety-alignment",
      "description": "AI safety and content moderation. Includes Constitutional AI (self-improvement training), LlamaGuard (input/output moderation), and NeMo Guardrails (runtime safety with Colang DSL). Use when implementing safety filters, content moderation, or jailbreak detection."
    },
    {
      "name": "distributed-training",
      "source": "./08-distributed-training",
      "description": "Multi-GPU and multi-node training. Includes DeepSpeed (ZeRO stages), PyTorch FSDP (native sharding), Accelerate (4-line distributed), Megatron-Core (tensor/pipeline parallelism), PyTorch Lightning (Trainer class), and Ray Train (cluster orchestration). Use when training large models across GPUs."
    },
    {
      "name": "infrastructure",
      "source": "./09-infrastructure",
      "description": "GPU cloud and compute orchestration. Includes Modal (serverless GPU), Lambda Labs (reserved instances), and SkyPilot (multi-cloud optimization). Use when deploying training jobs, managing GPU resources, or optimizing cloud costs."
    },
    {
      "name": "optimization",
      "source": "./10-optimization",
      "description": "Model optimization and quantization. Includes Flash Attention (2-4x speedup), bitsandbytes (8/4-bit quantization), GPTQ (4-bit post-training), AWQ (activation-aware), GGUF (llama.cpp format), and HQQ (calibration-free). Use when reducing memory, accelerating inference, or quantizing models."
    },
    {
      "name": "evaluation",
      "source": "./11-evaluation",
      "description": "LLM benchmarking and evaluation. Includes lm-evaluation-harness (60+ benchmarks like MMLU, HumanEval, GSM8K), BigCode Evaluation Harness (code models), and NeMo Evaluator (enterprise SDK). Use when benchmarking models or measuring performance on standard tasks."
    },
    {
      "name": "inference-serving",
      "source": "./12-inference-serving",
      "description": "Production LLM inference. Includes vLLM (PagedAttention, continuous batching), TensorRT-LLM (NVIDIA optimization), llama.cpp (CPU/Apple Silicon), and SGLang (structured generation, RadixAttention). Use when deploying models for production inference."
    },
    {
      "name": "mlops",
      "source": "./13-mlops",
      "description": "ML experiment tracking and lifecycle. Includes Weights & Biases (collaborative tracking, sweeps), MLflow (model registry, deployment), and TensorBoard (visualization, profiling). Use when tracking experiments, managing models, or visualizing training."
    },
    {
      "name": "agents",
      "source": "./14-agents",
      "description": "LLM agent frameworks. Includes LangChain (chains, agents, RAG), LlamaIndex (document Q&A, indexing), CrewAI (multi-agent orchestration), and AutoGPT (autonomous agents). Use when building chatbots, autonomous agents, or tool-using systems."
    },
    {
      "name": "rag",
      "source": "./15-rag",
      "description": "Retrieval-Augmented Generation. Includes Chroma (embedding database), FAISS (billion-scale vectors), Pinecone (managed vector DB), Qdrant (high-performance search), and Sentence Transformers (5000+ embedding models). Use when building semantic search or document retrieval systems."
    },
    {
      "name": "prompt-engineering",
      "source": "./16-prompt-engineering",
      "description": "Structured LLM outputs. Includes DSPy (automatic prompt optimization), Instructor (Pydantic validation), Guidance (regex/grammar control), and Outlines (guaranteed JSON/XML). Use when extracting structured data or constraining LLM outputs."
    },
    {
      "name": "observability",
      "source": "./17-observability",
      "description": "LLM application monitoring. Includes LangSmith (tracing, evaluation) and Phoenix (open-source observability). Use when debugging LLM apps, monitoring production systems, or evaluating outputs."
    },
    {
      "name": "multimodal",
      "source": "./18-multimodal",
      "description": "Vision, audio, and multimodal models. Includes CLIP (image-text matching), Whisper (speech recognition), LLaVA (visual chat), BLIP-2 (vision-language), Segment Anything (image segmentation), Stable Diffusion (text-to-image), and AudioCraft (music/audio generation). Use when working with images, audio, or multimodal tasks."
    },
    {
      "name": "emerging-techniques",
      "source": "./19-emerging-techniques",
      "description": "Advanced ML techniques. Includes MoE Training (Mixture of Experts), Model Merging (mergekit), Long Context (RoPE, YaRN), Speculative Decoding (1.5-3.6x speedup), Knowledge Distillation (model compression), and Model Pruning (Wanda, SparseGPT). Use when implementing cutting-edge optimization or architecture techniques."
    }
  ]
}
